{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrada: los datos ya descargados de RASFF y las salidas del \"Análisis_full_RASFF_Data\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../Datasets/full_RASFF_DATA.csv', sep=';', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datasets auxiliares (tratamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos = pd.read_csv('./../../Datasets/Lista_Productos.csv', header=0, index_col = 0)\n",
    "df_cat_productos = pd.read_csv('./../../Datasets/Lista_Categoria_Productos.csv', header=0, index_col = 0)\n",
    "df_amenazas = pd.read_csv('./../../Datasets/Lista_Amenazas.csv', header=0, index_col = 0)\n",
    "df_cat_amenazas = pd.read_csv('./../../Datasets/Lista_Categoria_Amenazas.csv', header=0, index_col = 0)\n",
    "df_repes = pd.read_csv('./../../Datasets/Lista_repeticion_paises.csv', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión de NaN a formato string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos la primera categoría de amenaza entre todas las posibles\n",
    "for index, row in df.iterrows():\n",
    "    row['HAZARDS_CAT'] = row['HAZARDS_CAT'].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las fechas que no nos interesan.\n",
    "fecha_maxima = \"2021\" #Primer año que no queremos coger.\n",
    "df = df.loc[df['DATE_CASE'] < fecha_maxima]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segun el dendograma visto anteriormente, vamos a agrupar las clases \n",
    "#\"labelling absent/incomplete/incorrect\" con \"packaging defective / incorrect\"\n",
    "#bajo una nueva clase llamada \"labelling absent/packaging defective/incorrect\"\n",
    "#Quitamos los nombres que contienen '/' para despues guardarlo como imagenes en carpetas.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(row['HAZARDS_CAT'] == \"labelling absent/incomplete/incorrect\" or row['HAZARDS_CAT'] == \"packaging defective / incorrect\"):\n",
    "        row['HAZARDS_CAT'] = \"packaging incorrect\"\n",
    "    if(row['HAZARDS_CAT'] == \"adulteration / fraud\"):\n",
    "        row['HAZARDS_CAT'] = \"adulteration or fraud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminamos los registros que no deseamos \n",
    "Las categorias obsoletas y los países con una tasa de participación inferior al 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paises_eliminar = df_repes.tail(72)\n",
    "cat_productos_eliminar = df_cat_productos.tail(16)\n",
    "cat_productos_eliminar = cat_productos_eliminar.append(df_cat_productos.iloc[-22])\n",
    "cat_productos_eliminar = cat_productos_eliminar.sort_values('Repeticiones', ascending = False)\n",
    "cat_amenazas_eliminar = df_cat_amenazas.tail(19) #Estaba en 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cat_amenazas.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los registros que contienen valores no interesantes para nuestro estudio, mirando en las columnas de interés en cada registro.\n",
    "\n",
    "#Guardamos las filas que no queremos coger en este datframe.\n",
    "df_eliminar = df.drop(df.index, inplace=False)\n",
    "\n",
    "#Buscamos esas filas \"inútiles\".\n",
    "for index, row in df.iterrows():\n",
    "    #Eliminamos los países invalidos.\n",
    "    for j in (row[\"COUNT_ORIGEN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_CONCERN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_DESTIN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los productos inválidos.\n",
    "    for j in (row[\"PROD_CAT\"].split(\",\")):\n",
    "        if (j in cat_productos_eliminar['Cat_Producto'].values):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos las categorías de amenazas inválidas.\n",
    "    for j in (row[\"HAZARDS_CAT\"].split(\",\")):\n",
    "        if (j in cat_amenazas_eliminar['Cat_Amenaza'].values \n",
    "            or row[\"HAZARDS_CAT\"] == \"\"\n",
    "            or row[\"HAZARDS_CAT\"] == \" \"\n",
    "            or row[\"HAZARDS_CAT\"] == \"not determined\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los registros que están vacios y no tienen información acerca de los paises.\n",
    "    if((row[\"COUNT_ORIGEN\"] == \" \" or row[\"COUNT_ORIGEN\"] == \"\") \n",
    "       and (row[\"COUNT_CONCERN\"] == \" \" or row[\"COUNT_CONCERN\"] == \"\") \n",
    "       and (row[\"COUNT_DESTIN\"] == \" \" or row[\"COUNT_DESTIN\"] == \"\")):\n",
    "        df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "        \n",
    "#Eliminamos por columna REF.\n",
    "cond = df['REF'].isin(df_eliminar['REF'])\n",
    "df.drop(df[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling y conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debido al gran desequilibrio entre clases, equilibramos los datos con técnicas de undersampling.\n",
    "num_max_clase = 1700      #Número de registros máximo por clase.\n",
    "\n",
    "#Creamos una lista con todos los dataframes según cada clase para hacfer el undersampling.\n",
    "ans = [pd.DataFrame(y) for x, y in df.groupby('HAZARDS_CAT', as_index=False)]\n",
    "\n",
    "\n",
    "#Escogemos la cantidad máxima de registros por cada clase.\n",
    "for i in range(len(ans)):\n",
    "    if(ans[i]['HAZARDS_CAT'].count() > num_max_clase):\n",
    "        ans[i] = ans[i].sample(n=num_max_clase)\n",
    "        \n",
    "#Reunimos todos en un único datframe (df otra vez).\n",
    "df = pd.concat(ans)\n",
    "#testRowData = pd.concat(listaClasesTest)\n",
    "\n",
    "#Hacemos un shuffle para mezclar las clases entre sí.\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos 2 datasets, uno con los parámetros a entrenar (df) y otro con la información relativa a cada registro (df_info)\n",
    "df_info = df[['REF','PRODUCT','HAZARDS','DATE_CASE','CLASSIF','TYPE','RISK_DECISION', 'ACTION_TAKEN','DISTRIBUTION_STAT','NOT_COUNTRY']].reset_index(drop = True)\n",
    "df = df[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n",
    "\n",
    "#De la misma forma con los datos del conjunto de test. QUEDA PENDIENTE HACER ESTO (TIENE QUE TENER MISMO FORMATO QUE LOS GRAFOS.)\n",
    "#testData = testRowData[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos el dataframe de las categorías de amenaza, que es lo que vamos a predecdf_nodes_featuressificar para\n",
    "#poder compararlos y hacer un entrenamiento supervisado. Empleando OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#Guardamos los valores temporalmente en y.\n",
    "y = pd.DataFrame(columns = []) \n",
    "y[\"y_value\"] = df['HAZARDS_CAT']\n",
    "y[\"y_code\"] = ord_enc.fit_transform(y[[\"y_value\"]])\n",
    "\n",
    "#Creamos un pequeño df con las conversiones, a modo de guía de qué codigo es qué valor.\n",
    "y_guide = y.groupby([\"y_value\",\"y_code\"]).sum()\n",
    "\n",
    "#Pasamos al formatpo de graph_labels, que es el que se emplea para las GCNNs.\n",
    "graph_labels = pd.Series(y[\"y_code\"], dtype = \"category\", name = \"Label\")\n",
    "\n",
    "df = df.drop('HAZARDS_CAT', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el one hot encoding de las categorías de los productos del df.\n",
    "x = pd.get_dummies(df['PROD_CAT'], prefix='PROD_CAT_')\n",
    "#display(x)\n",
    "#display(y.groupby(\"y_value\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos el producto que corresponde al peso de los grafos\n",
    "x = df['PROD_CAT'].astype('category').cat.codes\n",
    "x = pd.DataFrame(data=x, columns = ['weight'], dtype = np.float32)\n",
    "x['weight'] += 1 #Para que no haya un 0 como peso.\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación previa. Elegimos los países que vamos a usar (correspondiendo a los que hemos eliminado más arriba).\n",
    "paises = df_repes.head(159)['Pais'].sort_values().reset_index(drop = True)\n",
    "paises = paises[1:]          #Quitamos los huecos vacíos \" \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x158.\n",
    "df_nodes_features = pd.DataFrame(data = np.zeros((158, 158)), columns = paises)\n",
    "s = pd.Series(data=np.ones(158))\n",
    "np.fill_diagonal(df_nodes_features.values, s)\n",
    "\n",
    "#Se podría probar a que fuese 158x1, con un 0 o 1 si tiene conexión o no. \n",
    "#El pais se fija en el index y al propia red debería abstarer esa información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x1.\n",
    "\n",
    "#Lista con todos los Node Features.\n",
    "lista_df_node_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    arrayBase = np.zeros((158,1))\n",
    "    df_nodes = pd.DataFrame(data = arrayBase, columns = ['Feature'])\n",
    "\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "        paisesCO = \"\".split(\",\")\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "        paisesCC = \"\".split(\",\")\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "        paisesCD = \"\".split(\",\")\n",
    "\n",
    "    #Rellenamos con 1s donde exista el país\n",
    "    for i in range(len(paisesCO)):\n",
    "        if(LenCO != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(len(paisesCC)):\n",
    "        if(LenCC != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(len(paisesCD)):\n",
    "        if(LenCD != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCD[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "        \n",
    "    #Insertamos el df en la lista de df_node_features.\n",
    "    lista_df_node_features.append(df_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los Edge Features -> 1 por grafo. (Representa la info de cada conexión).\n",
    "#Columnas = Source, target y producto en one hot encoding. \n",
    "#Filas = nº de conexiones. \n",
    "\n",
    "#Lista con todos los Edge Features.\n",
    "lista_df_edge_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    df_edges = pd.DataFrame(columns = x.columns)\n",
    "    df_edges.insert (0, \"target\", np.nan)\n",
    "    df_edges.insert (0, \"source\", np.nan)\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "    #Creamos dos bucles anidados, para recorrer todos los source y linkarlos con todos los destinos. \n",
    "    #¡Esto funciona porque los índices de df_nodes siguen este mismo orden! \n",
    "    #Pero OJO! -> Existen 3 posibles casos: 1) LenCO = 0, 2) LenCC = 0 y 3) LenCD = 0. Teóricamente, habría que hacer 1->2->3.\n",
    "    #Si los 3 existen y son mayores que 0 se ejecutan los dos bucles.\n",
    "    \n",
    "    #Empezamos a rellenar el df, si existe LenCO y LenCC, lo rellenamos. \n",
    "    if(LenCO != 0 and LenCC != 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCC):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCC[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Si LenCD es 0, no pasa nada, el grafo ya está creado. Si sí que existe, seguimos rellenando el grafo.\n",
    "    if(LenCC != 0 and LenCD != 0):\n",
    "        for i in range(LenCC):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Por aquí solo va a entrar si no ha entrado en los otros dos.\n",
    "    if(LenCC == 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    \n",
    "    #Insertamos el df en la lista de df_edge_features.\n",
    "    lista_df_edge_features.append(df_edges)\n",
    "\n",
    "#Convertimos la columna de weights a numérico, para que no de errores al crear los grafos de Stellargraph.\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    lista_df_edge_features[i].weight = pd.to_numeric(lista_df_edge_features[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROD_CAT         meat and meat products (other than poultry)\n",
       "COUNT_ORIGEN                                          Poland\n",
       "COUNT_CONCERN                                               \n",
       "COUNT_DESTIN                                         Denmark\n",
       "Name: 5, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "..       ...\n",
       "153      0.0\n",
       "154      0.0\n",
       "155      0.0\n",
       "156      0.0\n",
       "157      0.0\n",
       "\n",
       "[158 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  target  weight\n",
       "0   116.0    39.0    17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplos para enseñar\n",
    "miniIndex = 5\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(df.iloc[miniIndex])\n",
    "display(lista_df_node_features[miniIndex])\n",
    "display(lista_df_edge_features[miniIndex])\n",
    "graph_labels[miniIndex]\n",
    "#y_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los grafos en NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui creamos los grafos de networkx\n",
    "lista_grafos_NetworkX = []\n",
    "\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    G = nx.DiGraph()\n",
    "    #Si queremos todos los nodos hacemos arange[1,158], de la siguiente forma coge solamente los nodos que tienen conexiones.\n",
    "    G.add_nodes_from(lista_df_node_features[i].index[lista_df_node_features[i]['Feature']==1].tolist())\n",
    "    for j in range(len(lista_df_edge_features[i])):\n",
    "        source = int(lista_df_edge_features[i].iloc[j].source)\n",
    "        target = int(lista_df_edge_features[i].iloc[j].target)\n",
    "        weight = lista_df_edge_features[i].iloc[j].weight\n",
    "        G.add_edge(source,target,weight = weight)\n",
    "    lista_grafos_NetworkX.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['PROD_CAT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dibujamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario apra guardar las posiciones de los 158 paises en la imagen a representar.\n",
    "dict_posiciones = {}\n",
    "sqr_div = int(math.ceil(math.sqrt(len(paises))))\n",
    "\n",
    "for i in range(len(paises)):\n",
    "    dict_posiciones[i] = (math.floor(i/sqr_div),i%sqr_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Array de colores según el producto (Pesos):\n",
    "colores = ['xkcd:blue','xkcd:pink','xkcd:light green','xkcd:tan','xkcd:grey','xkcd:light blue','xkcd:light purple','xkcd:cyan','xkcd:turquoise',\n",
    "           'xkcd:baby puke green','xkcd:salmon','xkcd:dark pink','xkcd:mustard','xkcd:periwinkle','xkcd:pale blue','xkcd:burnt orange','xkcd:pea green',\n",
    "           'xkcd:goldenrod','xkcd:light grey','xkcd:greenish','xkcd:dusty rose','xkcd:scarlet','xkcd:dark beige','xkcd:neon blue',\n",
    "           'xkcd:baby shit brown','xkcd:burnt red','xkcd:muddy green','xkcd:bronze','xkcd:vermillion','xkcd:dusky purple',\n",
    "           'xkcd:dark mint','xkcd:warm brown','xkcd:pastel purple', 'xkcd:slate blue']\n",
    "len(colores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array para contar y validar la clase de la imagen y poner en las distintas carpetas.\n",
    "#Jerarquía:\n",
    "#->Graph_Images-->TRAIN -----> Clases creadas dinámicamente\n",
    "#              -->TEST  -----> Clases creadas dinámicamente\n",
    "count_Array = np.zeros((len(y['y_code'].unique())))\n",
    "#[0,0,0,0]\n",
    "#[Clase1, Clase2, ...]\n",
    "#Insertamos las 30 primeras imágenes de cada clase en el conjunto de test.\n",
    "numImagenesTestPorClase = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creación de las imágenes de los grafos. Tarda un ratillo.\n",
    "\n",
    "#Borramos el contenido de Graph_Images por si cmabian las clases.\n",
    "#MUCHO CUIDADO CON NO BORRAR LA CARPETA '.'\n",
    "try:\n",
    "    shutil.rmtree('./Graph_Images/')\n",
    "except OSError as e:\n",
    "    print(\"Error:./Graph_Images: %s\" % (e.strerror))\n",
    "\n",
    "#Creamos las imágenes\n",
    "for i in range(len(lista_grafos_NetworkX)):\n",
    "    #-----------------Dibujamos un grafo--------------------\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #Cogemos las posiciones que nos interesan y para ello generamos un diccionario personalizado con las posiciones:\n",
    "    dict_fixed_positions = {}\n",
    "\n",
    "    for j in list(lista_grafos_NetworkX[i].nodes):\n",
    "        dict_fixed_positions[int(j)] = dict_posiciones[int(j)]\n",
    "\n",
    "    fixed_positions = dict_fixed_positions\n",
    "    fixed_nodes = fixed_positions.keys()\n",
    "    pos = nx.spring_layout(lista_grafos_NetworkX[i], pos=fixed_positions, fixed = fixed_nodes)\n",
    "\n",
    "    #Dibujamos el grafo.\n",
    "    nx.draw_networkx(lista_grafos_NetworkX[i], pos, with_labels=True, node_size = 180, font_size=9)\n",
    "\n",
    "    #Scale\n",
    "    plt.xlim(-1, 13)\n",
    "    plt.ylim(-1, 13)\n",
    "\n",
    "    #Set limits to be shown.\n",
    "    #limits=plt.axis('on') # turns on axis\n",
    "    #ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "\n",
    "    #Cogemos el índice correspondiente al peso del grafo, representando el producto.\n",
    "    ax.set_facecolor(colores[int(x.weight[i]-1)])\n",
    "    \n",
    "    #Miramos la clase del grafo y guardamos la imagen.\n",
    "    if(count_Array[int(y['y_code'][i])] < numImagenesTestPorClase):\n",
    "        #Miramos si el path existe, sino lo creamos\n",
    "        if not os.path.exists('./Graph_Images/TEST/'+y['y_value'][i]):\n",
    "            os.makedirs('./Graph_Images/TEST/'+y['y_value'][i])\n",
    "        #Guardamos el archivo\n",
    "        plt.savefig('./Graph_Images/TEST/'+y['y_value'][i]+'/Graph'+str(int(count_Array[int(y['y_code'][i])]))+'.png')\n",
    "    else:\n",
    "        #Miramos si el path existe, sino lo creamos\n",
    "        if not os.path.exists('./Graph_Images/TRAIN/'+y['y_value'][i]):\n",
    "            os.makedirs('./Graph_Images/TRAIN/'+y['y_value'][i])\n",
    "        #Guardamos el archivo\n",
    "        plt.savefig('./Graph_Images/TRAIN/'+y['y_value'][i]+'/Graph'+str(int(count_Array[int(y['y_code'][i])]))+'.png')\n",
    "\n",
    "    #Actualizamos el array contador de imagenes por clase.\n",
    "    count_Array[int(y['y_code'][i])] = count_Array[int(y['y_code'][i])] + 1\n",
    "    \n",
    "    #Close the plot and dont show it.\n",
    "    plt.close()\n",
    "\n",
    "    #Draw the plot if we wanna see it in console.\n",
    "    #plt.draw() \n",
    "    #plt.show() \n",
    "    #-----------------Fin del dibujado de un grafo--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'y' (DataFrame)\n",
      "Stored 'colores' (list)\n"
     ]
    }
   ],
   "source": [
    "%store y\n",
    "%store colores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
