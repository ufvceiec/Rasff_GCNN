{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrada: los datos ya descargados de RASFF y las salidas del \"Análisis_full_RASFF_Data\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./full_RASFF_DATA.csv', sep=';', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datasets auxiliares (tratamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos = pd.read_csv('./Lista_Productos.csv', header=0, index_col = 0)\n",
    "df_cat_productos = pd.read_csv('./Lista_Categoria_Productos.csv', header=0, index_col = 0)\n",
    "df_amenazas = pd.read_csv('./Lista_Amenazas.csv', header=0, index_col = 0)\n",
    "df_cat_amenazas = pd.read_csv('./Lista_Categoria_Amenazas.csv', header=0, index_col = 0)\n",
    "df_repes = pd.read_csv('./Lista_repeticion_paises.csv', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión de NaN a formato string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos la primera categoría de amenaza entre todas las posibles\n",
    "for index, row in df.iterrows():\n",
    "    row['HAZARDS_CAT'] = row['HAZARDS_CAT'].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las fechas que no nos interesan.\n",
    "fecha_maxima = \"2021\" #Primer año que no queremos coger.\n",
    "df = df.loc[df['DATE_CASE'] < fecha_maxima]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segun el dendograma visto anteriormente, vamos a agrupar las clases \n",
    "#\"labelling absent/incomplete/incorrect\" con \"packaging defective / incorrect\"\n",
    "#bajo una nueva clase llamada \"labelling absent/packaging defective/incorrect\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(row['HAZARDS_CAT'] == \"labelling absent/incomplete/incorrect\" or row['HAZARDS_CAT'] == \"packaging defective / incorrect\"):\n",
    "        row['HAZARDS_CAT'] = \"labelling absent/packaging defective/incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminamos los registros que no deseamos \n",
    "Las categorias obsoletas y los países con una tasa de participación inferior al 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paises_eliminar = df_repes.tail(72)\n",
    "cat_productos_eliminar = df_cat_productos.tail(8)\n",
    "cat_amenazas_eliminar = df_cat_amenazas.tail(33) #Estaba en 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cat_amenazas.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los registros que contienen valores no interesantes para nuestro estudio, mirando en las columnas de interés en cada registro.\n",
    "\n",
    "#Guardamos las filas que no queremos coger en este datframe.\n",
    "df_eliminar = df.drop(df.index, inplace=False)\n",
    "\n",
    "#Buscamos esas filas \"inútiles\".\n",
    "for index, row in df.iterrows():\n",
    "    #Eliminamos los países invalidos.\n",
    "    for j in (row[\"COUNT_ORIGEN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_CONCERN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_DESTIN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los productos inválidos.\n",
    "    for j in (row[\"PROD_CAT\"].split(\",\")):\n",
    "        if (j in cat_productos_eliminar['Cat_Producto'].values):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos las categorías de amenazas inválidas.\n",
    "    for j in (row[\"HAZARDS_CAT\"].split(\",\")):\n",
    "        if (j in cat_amenazas_eliminar['Cat_Amenaza'].values \n",
    "            or row[\"HAZARDS_CAT\"] == \"\"\n",
    "            or row[\"HAZARDS_CAT\"] == \" \"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los registros que están vacios y no tienen información acerca de los paises.\n",
    "    if((row[\"COUNT_ORIGEN\"] == \" \" or row[\"COUNT_ORIGEN\"] == \"\") \n",
    "       and (row[\"COUNT_CONCERN\"] == \" \" or row[\"COUNT_CONCERN\"] == \"\") \n",
    "       and (row[\"COUNT_DESTIN\"] == \" \" or row[\"COUNT_DESTIN\"] == \"\")):\n",
    "        df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "        \n",
    "#Eliminamos por columna REF.\n",
    "cond = df['REF'].isin(df_eliminar['REF'])\n",
    "df.drop(df[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling y conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debido al gran desequilibrio entre clases, equilibramos los datos con técnicas de undersampling.\n",
    "num_max_clase = 1700      #Número de registros máximo por clase.\n",
    "num_registros_test = 200  #Número de registros por clase para el conjunto de testeo.\n",
    "\n",
    "#Creamos una lista con todos los dataframes según cada clase para hacfer el undersampling.\n",
    "ans = [pd.DataFrame(y) for x, y in df.groupby('HAZARDS_CAT', as_index=False)]\n",
    "\n",
    "#Creamos el conjunto de test por clases (sin seleccionar las columnas deseadas). Seleccionando 200 casos de todas las clases de forma random.\n",
    "listaClasesTest = ans.copy()\n",
    "for i in range(len(listaClasesTest)):\n",
    "    listaClasesTest[i] = listaClasesTest[i].sample(n=num_registros_test)\n",
    "\n",
    "#Escogemos la cantidad máxima de registros por cada clase.\n",
    "for i in range(len(ans)):\n",
    "    if(ans[i]['HAZARDS_CAT'].count() > num_max_clase):\n",
    "        ans[i] = ans[i].sample(n=num_max_clase)\n",
    "        \n",
    "#Reunimos todos en un único datframe (df otra vez).\n",
    "df = pd.concat(ans)\n",
    "testRowData = pd.concat(listaClasesTest)\n",
    "\n",
    "#Hacemos un shuffle para mezclar las clases entre sí.\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos 2 datasets, uno con los parámetros a entrenar (df) y otro con la información relativa a cada registro (df_info)\n",
    "df_info = df[['REF','PRODUCT','HAZARDS','DATE_CASE','CLASSIF','TYPE','RISK_DECISION', 'ACTION_TAKEN','DISTRIBUTION_STAT','NOT_COUNTRY']].reset_index(drop = True)\n",
    "df = df[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n",
    "\n",
    "#De la misma forma con los datos del conjunto de test. QUEDA PENDIENTE HACER ESTO (TIENE QUE TENER MISMO FORMATO QUE LOS GRAFOS.)\n",
    "testData = testRowData[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos el dataframe de las categorías de amenaza, que es lo que vamos a predecdf_nodes_featuressificar para\n",
    "#poder compararlos y hacer un entrenamiento supervisado. Empleando OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#Guardamos los valores temporalmente en y.\n",
    "y = pd.DataFrame(columns = []) \n",
    "y[\"y_value\"] = df['HAZARDS_CAT']\n",
    "y[\"y_code\"] = ord_enc.fit_transform(y[[\"y_value\"]])\n",
    "\n",
    "#Creamos un pequeño df con las conversiones, a modo de guía de qué codigo es qué valor.\n",
    "y_guide = y.groupby([\"y_value\",\"y_code\"]).sum()\n",
    "\n",
    "#Pasamos al formatpo de graph_labels, que es el que se emplea para las GCNNs.\n",
    "graph_labels = pd.Series(y[\"y_code\"], dtype = \"category\", name = \"Label\")\n",
    "\n",
    "df = df.drop('HAZARDS_CAT', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_value</th>\n",
       "      <th>y_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pathogenic micro-organisms</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pathogenic micro-organisms</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pathogenic micro-organisms</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mycotoxins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pathogenic micro-organisms</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>mycotoxins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>labelling absent/packaging defective/incorrect</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>mycotoxins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>mycotoxins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>labelling absent/packaging defective/incorrect</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             y_value  y_code\n",
       "0                         pathogenic micro-organisms     2.0\n",
       "1                         pathogenic micro-organisms     2.0\n",
       "2                         pathogenic micro-organisms     2.0\n",
       "3                                         mycotoxins     1.0\n",
       "4                         pathogenic micro-organisms     2.0\n",
       "...                                              ...     ...\n",
       "4199                                      mycotoxins     1.0\n",
       "4200  labelling absent/packaging defective/incorrect     0.0\n",
       "4201                                      mycotoxins     1.0\n",
       "4202                                      mycotoxins     1.0\n",
       "4203  labelling absent/packaging defective/incorrect     0.0\n",
       "\n",
       "[4204 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el one hot encoding de las categorías de los productos del df.\n",
    "x = pd.get_dummies(df['PROD_CAT'], prefix='PROD_CAT_')\n",
    "#display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labelling absent/packaging defective/incorrect</th>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mycotoxins</th>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pathogenic micro-organisms</th>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                y_code\n",
       "y_value                                               \n",
       "labelling absent/packaging defective/incorrect     804\n",
       "mycotoxins                                        1700\n",
       "pathogenic micro-organisms                        1700"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.groupby(\"y_value\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos el producto que corresponde al peso de los grafos\n",
    "x = df['PROD_CAT'].astype('category').cat.codes\n",
    "x = pd.DataFrame(data=x, columns = ['weight'], dtype = np.float32)\n",
    "x['weight'] += 1 #Para que no haya un 0 como peso.\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación previa. Elegimos los países que vamos a usar (correspondiendo a los que hemos eliminado más arriba).\n",
    "paises = df_repes.head(159)['Pais'].sort_values().reset_index(drop = True)\n",
    "paises = paises[1:]          #Quitamos los huecos vacíos \" \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x158.\n",
    "df_nodes_features = pd.DataFrame(data = np.zeros((158, 158)), columns = paises)\n",
    "s = pd.Series(data=np.ones(158))\n",
    "np.fill_diagonal(df_nodes_features.values, s)\n",
    "\n",
    "#Se podría probar a que fuese 158x1, con un 0 o 1 si tiene conexión o no. \n",
    "#El pais se fija en el index y al propia red debería abstarer esa información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x1.\n",
    "\n",
    "#Lista con todos los Node Features.\n",
    "lista_df_node_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    arrayBase = np.zeros((158,1))\n",
    "    df_nodes = pd.DataFrame(data = arrayBase, columns = ['Feature'])\n",
    "\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "        paisesCO = \"\".split(\",\")\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "        paisesCC = \"\".split(\",\")\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "        paisesCD = \"\".split(\",\")\n",
    "\n",
    "    #Rellenamos con 1s donde exista el país\n",
    "    for i in range(len(paisesCO)):\n",
    "        if(LenCO != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(len(paisesCC)):\n",
    "        if(LenCC != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(len(paisesCD)):\n",
    "        if(LenCD != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCD[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "        \n",
    "    #Insertamos el df en la lista de df_node_features.\n",
    "    lista_df_node_features.append(df_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los Edge Features -> 1 por grafo. (Representa la info de cada conexión).\n",
    "#Columnas = Source, target y producto en one hot encoding. \n",
    "#Filas = nº de conexiones. \n",
    "\n",
    "#Lista con todos los Edge Features.\n",
    "lista_df_edge_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    df_edges = pd.DataFrame(columns = x.columns)\n",
    "    df_edges.insert (0, \"target\", np.nan)\n",
    "    df_edges.insert (0, \"source\", np.nan)\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "    #Creamos dos bucles anidados, para recorrer todos los source y linkarlos con todos los destinos. \n",
    "    #¡Esto funciona porque los índices de df_nodes siguen este mismo orden! \n",
    "    #Pero OJO! -> Existen 3 posibles casos: 1) LenCO = 0, 2) LenCC = 0 y 3) LenCD = 0. Teóricamente, habría que hacer 1->2->3.\n",
    "    #Si los 3 existen y son mayores que 0 se ejecutan los dos bucles.\n",
    "    \n",
    "    #Empezamos a rellenar el df, si existe LenCO y LenCC, lo rellenamos. \n",
    "    if(LenCO != 0 and LenCC != 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCC):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCC[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Si LenCD es 0, no pasa nada, el grafo ya está creado. Si sí que existe, seguimos rellenando el grafo.\n",
    "    if(LenCC != 0 and LenCD != 0):\n",
    "        for i in range(LenCC):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Por aquí solo va a entrar si no ha entrado en los otros dos.\n",
    "    if(LenCC == 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    \n",
    "    #Insertamos el df en la lista de df_edge_features.\n",
    "    lista_df_edge_features.append(df_edges)\n",
    "\n",
    "#Convertimos la columna de weights a numérico, para que no de errores al crear los grafos de Stellargraph.\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    lista_df_edge_features[i].weight = pd.to_numeric(lista_df_edge_features[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROD_CAT         prepared dishes and snacks\n",
       "COUNT_ORIGEN                        Belgium\n",
       "COUNT_CONCERN                    Luxembourg\n",
       "COUNT_DESTIN                               \n",
       "Name: 5, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "..       ...\n",
       "153      0.0\n",
       "154      0.0\n",
       "155      0.0\n",
       "156      0.0\n",
       "157      0.0\n",
       "\n",
       "[158 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  target  weight\n",
       "0    12.0    87.0    30.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplos para enseñar\n",
    "miniIndex = 5\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(df.iloc[miniIndex])\n",
    "display(lista_df_node_features[miniIndex])\n",
    "display(lista_df_edge_features[miniIndex])\n",
    "graph_labels[miniIndex]\n",
    "#y_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los grafos en NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lista_df_edge_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1eff4d087aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlista_grafos_NetworkX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_df_edge_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#Si queremos todos los ndoos hacemos arange[1,158], de la siguiente forma coge solamente los nodos que tienen conexiones.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lista_df_edge_features' is not defined"
     ]
    }
   ],
   "source": [
    "#Aqui creamos los grafos de networkx\n",
    "lista_grafos_NetworkX = []\n",
    "\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    G = nx.DiGraph()\n",
    "    #Si queremos todos los ndoos hacemos arange[1,158], de la siguiente forma coge solamente los nodos que tienen conexiones.\n",
    "    G.add_nodes_from(np.arange(1,158))\n",
    "    for j in range(len(lista_df_edge_features[i])):\n",
    "        source = lista_df_edge_features[i].iloc[j].source\n",
    "        target = lista_df_edge_features[i].iloc[j].target\n",
    "        G.add_edges_from([(source,target)])\n",
    "    lista_grafos_NetworkX.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista con todos los StellarGraphs desde Networkx\n",
    "lista_grafos = []\n",
    "\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    s = StellarGraph.from_networkx(lista_grafos_NetworkX[i], node_features = lista_df_node_features[i])\n",
    "    lista_grafos.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarDiGraph: Directed multigraph\n",
      " Nodes: 158, Edges: 1\n",
      "\n",
      " Node types:\n",
      "  default: [158]\n",
      "    Features: float32 vector, length 1\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [1]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "#Visualizamos info de un grafo\n",
    "print(lista_grafos[5].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4204.0</td>\n",
       "      <td>4204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>158.0</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>158.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>158.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nodes   edges\n",
       "count  4204.0  4204.0\n",
       "mean    158.0     1.6\n",
       "std       0.0     1.5\n",
       "min     158.0     0.0\n",
       "25%     158.0     1.0\n",
       "50%     158.0     1.0\n",
       "75%     158.0     2.0\n",
       "max     158.0    28.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Información acerca de los grafos obtenidos.\n",
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges()) for g in lista_grafos],\n",
    "    columns=[\"nodes\", \"edges\"],\n",
    ")\n",
    "summary.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lista_grafos' (list)\n",
      "Stored 'graph_labels' (Series)\n",
      "Stored 'y_guide' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store lista_grafos\n",
    "%store graph_labels\n",
    "%store y_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cortamos aquí el script y creamos otro nuevo para hacer el workflow (entrenamiento)\n",
    "#Usamos %store, sino JSON y sino Pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers as opt\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo que ignora la matriz de edges, hay que ver si se puede poner esa info en los pesos de las conexiones. \n",
    "#Ver si se puede usar otro generator. (No es viable).\n",
    "#Soluciones -> Pasar el producto al nodo, o probar con las conexiones.\n",
    "#\n",
    "generator = PaddedGraphGenerator(graphs=lista_grafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  #The number of rows for the output tensor\n",
    "layer_sizes = [40, 40, 40, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=k,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "x_inp, x_out = dgcnn_model.in_out_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=32, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "\n",
    "model.compile(\n",
    "    optimizer= opt.Adam(lr=0.0001), loss=losses.BinaryCrossentropy(), metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    lista_grafos, train_size=0.75, test_size=None, stratify=graph_labels, #graph_labels, train_size=0.75, test_size=None, stratify=graph_labels,\n",
    ")\n",
    "#Randomizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = PaddedGraphGenerator(graphs=lista_grafos)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    train_graphs,\n",
    "    targets=graph_labels[:2550],\n",
    "    weighted = True,\n",
    "    batch_size=10, \n",
    "    symmetric_normalization=True, #True?\n",
    "    #seed = 0,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    test_graphs,\n",
    "    targets=graph_labels[2550:],\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=True,\n",
    "    weighted = True,\n",
    "    #seed = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_graphs\n",
    "#print(lista_grafos[0].info())\n",
    "#list(train_graphs.index - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = 0\n",
    "prediction = model.predict(test_gen)\n",
    "display(prediction[1])\n",
    "display(prediction[2])\n",
    "display(prediction[3])\n",
    "display(prediction[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Creamos los Node Features -> 1 por grafo. Columnas = Países one hot encoding. Filas = nº de nodos. (Representa la info de cada nodo).\n",
    "#Para ello, recorremos todo el dataset y creamos un grafo de nodos\n",
    "\n",
    "#Lista con todos los Node Features.\n",
    "lista_df_nodes_features = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df_nodes = pd.DataFrame(columns = paises)\n",
    "    #Variable para guardar todos los países.\n",
    "    paises1 = \"\"\n",
    "    #Rellenamos la variable.\n",
    "    if(isinstance(row['COUNT_ORIGEN'], str)):\n",
    "        if(row['COUNT_ORIGEN'] != \"\"):\n",
    "            paises1 = paises1 + row['COUNT_ORIGEN'] \n",
    "    if(isinstance(row['COUNT_CONCERN'], str)):\n",
    "        if(row['COUNT_CONCERN'] != \"\"):\n",
    "            paises1 = paises1 + \",\" + row['COUNT_CONCERN']\n",
    "    if(isinstance(row['COUNT_DESTIN'], str)):\n",
    "        if(row['COUNT_DESTIN'] != \"\"):\n",
    "            paises1 = paises1 + \",\" + row['COUNT_DESTIN']\n",
    "    #Por cada país en el registro, creo un nodo.\n",
    "    for j in paises1.split(\",\"):\n",
    "        if(j != \"\" and j!= \" \"):\n",
    "            if(not j in paises.values):\n",
    "                display(j)\n",
    "                display(row)\n",
    "            else:\n",
    "                #Cogemos la posición del país\n",
    "                pos = paises[paises == j].index[0]\n",
    "                #Creo los datos de la row, como una serie\n",
    "                serie = pd.Series([0] * (pos-1) + [1] * 1 + [0] * (len(paises) - pos))\n",
    "                #Inserto la row en el df_nodes\n",
    "                df_nodes.loc[df_nodes.shape[0]] = serie.values\n",
    "    #Insertamos el df en la lista de df_nodes_features.\n",
    "    lista_df_nodes_features.append(df_nodes)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Creamos los Edge Features -> 1 por grafo. (Representa la info de cada conexión).\n",
    "#Columnas = Source, target y producto en one hot encoding. \n",
    "#Filas = nº de conexiones. \n",
    "\n",
    "#Lista con todos los Edge Features.\n",
    "lista_df_edge_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    df_edges = pd.DataFrame(columns = x.columns)\n",
    "    df_edges.insert (0, \"target\", np.nan)\n",
    "    df_edges.insert (0, \"source\", np.nan)\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "    else:\n",
    "        LenCO = 0\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "    else:\n",
    "        LenCC = 0\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "    #Creamos dos bucles anidados, para recorrer todos los source y linkarlos con todos los destinos. \n",
    "    #¡Esto funciona porque los índices de df_nodes siguen este mismo orden! \n",
    "    #Pero OJO! -> Existen 3 posibles casos: 1) LenCO = 0, 2) LenCC = 0 y 3) LenCD = 0. Teóricamente, habría que hacer 1->2->3.\n",
    "    #Si los 3 existen y son mayores que 0 se ejecutan los dos bucles.\n",
    "    \n",
    "    #Empezamos a rellenar el df, si existe LenCO y LenCC, lo rellenamos. \n",
    "    if(LenCO != 0 and LenCC != 0):\n",
    "        for i in range(LenCO):\n",
    "            for j in range(LenCC):\n",
    "                serie = pd.Series([i] + [j+LenCO]).append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    #Si LenCD es 0, no pasa nada, el grafo ya está creado. Si sí que existe, seguimos rellenando el grafo.\n",
    "    if(LenCC != 0 and LenCD != 0):\n",
    "        for i in range(LenCC):\n",
    "            for j in range(LenCD):\n",
    "                serie = pd.Series([i+LenCO] + [j+LenCO+LenCC]).append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    #Por aquí solo va a entrar si no ha entrado en los otros dos.\n",
    "    if(LenCC == 0):\n",
    "        for i in range(LenCO):\n",
    "            for j in range(LenCD):\n",
    "                serie = pd.Series([i] + [j+LenCO]).append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    \n",
    "    #Insertamos el df en la lista de df_edge_features.\n",
    "    lista_df_edge_features.append(df_edges)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
