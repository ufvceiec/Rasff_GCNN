{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrada: los datos ya descargados de RASFF y las salidas del \"Análisis_full_RASFF_Data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras import losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../Datasets/full_RASFF_DATA.csv', sep=';', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datasets auxiliares (tratamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos = pd.read_csv('./../../Datasets/Lista_Productos.csv', header=0, index_col = 0)\n",
    "df_cat_productos = pd.read_csv('./../../Datasets/Lista_Categoria_Productos.csv', header=0, index_col = 0)\n",
    "df_amenazas = pd.read_csv('./../../Datasets/Lista_Amenazas.csv', header=0, index_col = 0)\n",
    "df_cat_amenazas = pd.read_csv('./../../Datasets/Lista_Categoria_Amenazas.csv', header=0, index_col = 0)\n",
    "df_repes = pd.read_csv('./../../Datasets/Lista_repeticion_paises.csv', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión de NaN a formato string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos la primera categoría de amenaza entre todas las posibles\n",
    "for index, row in df.iterrows():\n",
    "    row['HAZARDS_CAT'] = row['HAZARDS_CAT'].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las fechas que no nos interesan.\n",
    "fecha_maxima = \"2021\" #Primer año que no queremos coger.\n",
    "df = df.loc[df['DATE_CASE'] < fecha_maxima]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segun el dendograma visto anteriormente, vamos a agrupar las clases \n",
    "#\"labelling absent/incomplete/incorrect\" con \"packaging defective / incorrect\"\n",
    "#bajo una nueva clase llamada \"labelling absent/packaging defective/incorrect\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(row['HAZARDS_CAT'] == \"labelling absent/incomplete/incorrect\" or row['HAZARDS_CAT'] == \"packaging defective / incorrect\"):\n",
    "        row['HAZARDS_CAT'] = \"labelling absent/packaging defective/incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminamos los registros que no deseamos \n",
    "Las categorias obsoletas y los países con una tasa de participación inferior al 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paises_eliminar = df_repes.tail(72)\n",
    "cat_productos_eliminar = df_cat_productos.tail(16)\n",
    "cat_productos_eliminar = cat_productos_eliminar.append(df_cat_productos.iloc[-22])\n",
    "cat_productos_eliminar = cat_productos_eliminar.sort_values('Repeticiones', ascending = False)\n",
    "cat_amenazas_eliminar = df_cat_amenazas.tail(19) #Estaba en 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cat_amenazas.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los registros que contienen valores no interesantes para nuestro estudio, mirando en las columnas de interés en cada registro.\n",
    "\n",
    "#Guardamos las filas que no queremos coger en este datframe.\n",
    "df_eliminar = df.drop(df.index, inplace=False)\n",
    "\n",
    "#Buscamos esas filas \"inútiles\".\n",
    "for index, row in df.iterrows():\n",
    "    #Eliminamos los países invalidos.\n",
    "    for j in (row[\"COUNT_ORIGEN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_CONCERN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    for j in (row[\"COUNT_DESTIN\"].split(\",\")):\n",
    "        if (j in paises_eliminar['Pais'].values or j == \"INFOSAN\" or j == \"Commission Services\"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los productos inválidos.\n",
    "    for j in (row[\"PROD_CAT\"].split(\",\")):\n",
    "        if (j in cat_productos_eliminar['Cat_Producto'].values):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos las categorías de amenazas inválidas.\n",
    "    for j in (row[\"HAZARDS_CAT\"].split(\",\")):\n",
    "        if (j in cat_amenazas_eliminar['Cat_Amenaza'].values \n",
    "            or row[\"HAZARDS_CAT\"] == \"\"\n",
    "            or row[\"HAZARDS_CAT\"] == \" \"):\n",
    "            df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "    #Eliminamos los registros que están vacios y no tienen información acerca de los paises.\n",
    "    if((row[\"COUNT_ORIGEN\"] == \" \" or row[\"COUNT_ORIGEN\"] == \"\") \n",
    "       and (row[\"COUNT_CONCERN\"] == \" \" or row[\"COUNT_CONCERN\"] == \"\") \n",
    "       and (row[\"COUNT_DESTIN\"] == \" \" or row[\"COUNT_DESTIN\"] == \"\")):\n",
    "        df_eliminar.loc[df_eliminar.shape[0]] = row\n",
    "        \n",
    "#Eliminamos por columna REF.\n",
    "cond = df['REF'].isin(df_eliminar['REF'])\n",
    "df.drop(df[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling y conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debido al gran desequilibrio entre clases, equilibramos los datos con técnicas de undersampling.\n",
    "num_max_clase = 1700      #Número de registros máximo por clase.\n",
    "num_registros_test = 200  #Número de registros por clase para el conjunto de testeo.\n",
    "\n",
    "#Creamos una lista con todos los dataframes según cada clase para hacfer el undersampling.\n",
    "ans = [pd.DataFrame(y) for x, y in df.groupby('HAZARDS_CAT', as_index=False)]\n",
    "\n",
    "#Creamos el conjunto de test por clases (sin seleccionar las columnas deseadas). Seleccionando 200 casos de todas las clases de forma random.\n",
    "listaClasesTest = ans.copy()\n",
    "for i in range(len(listaClasesTest)):\n",
    "    listaClasesTest[i] = listaClasesTest[i].sample(n=num_registros_test)\n",
    "\n",
    "#Escogemos la cantidad máxima de registros por cada clase.\n",
    "for i in range(len(ans)):\n",
    "    if(ans[i]['HAZARDS_CAT'].count() > num_max_clase):\n",
    "        ans[i] = ans[i].sample(n=num_max_clase)\n",
    "        \n",
    "#Reunimos todos en un único datframe (df otra vez).\n",
    "df = pd.concat(ans)\n",
    "testRowData = pd.concat(listaClasesTest)\n",
    "\n",
    "#Hacemos un shuffle para mezclar las clases entre sí.\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos 2 datasets, uno con los parámetros a entrenar (df) y otro con la información relativa a cada registro (df_info)\n",
    "df_info = df[['REF','PRODUCT','HAZARDS','DATE_CASE','CLASSIF','TYPE','RISK_DECISION', 'ACTION_TAKEN','DISTRIBUTION_STAT','NOT_COUNTRY']].reset_index(drop = True)\n",
    "df = df[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n",
    "\n",
    "#De la misma forma con los datos del conjunto de test. QUEDA PENDIENTE HACER ESTO (TIENE QUE TENER MISMO FORMATO QUE LOS GRAFOS.)\n",
    "testData = testRowData[['PROD_CAT','HAZARDS_CAT','COUNT_ORIGEN','COUNT_CONCERN','COUNT_DESTIN']].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos el dataframe de las categorías de amenaza, que es lo que vamos a predecdf_nodes_featuressificar para\n",
    "#poder compararlos y hacer un entrenamiento supervisado. Empleando OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#Guardamos los valores temporalmente en y.\n",
    "y = pd.DataFrame(columns = []) \n",
    "y[\"y_value\"] = df['HAZARDS_CAT']\n",
    "y[\"y_code\"] = ord_enc.fit_transform(y[[\"y_value\"]])\n",
    "\n",
    "#Creamos un pequeño df con las conversiones, a modo de guía de qué codigo es qué valor.\n",
    "y_guide = y.groupby([\"y_value\",\"y_code\"]).sum()\n",
    "\n",
    "#Pasamos al formatpo de graph_labels, que es el que se emplea para las GCNNs.\n",
    "graph_labels = pd.Series(y[\"y_code\"], dtype = \"category\", name = \"Label\")\n",
    "\n",
    "df = df.drop('HAZARDS_CAT', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el one hot encoding de las categorías de los productos del df.\n",
    "x = pd.get_dummies(df['PROD_CAT'], prefix='PROD_CAT_')\n",
    "#display(x)\n",
    "#display(y.groupby(\"y_value\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos el producto que corresponde al peso de los grafos\n",
    "x = df['PROD_CAT'].astype('category').cat.codes\n",
    "x = pd.DataFrame(data=x, columns = ['weight'], dtype = np.float32)\n",
    "x['weight'] += 1 #Para que no haya un 0 como peso.\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación previa. Elegimos los países que vamos a usar (correspondiendo a los que hemos eliminado más arriba).\n",
    "paises = df_repes.head(159)['Pais'].sort_values().reset_index(drop = True)\n",
    "paises = paises[1:]          #Quitamos los huecos vacíos \" \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x158.\n",
    "df_nodes_features = pd.DataFrame(data = np.zeros((158, 158)), columns = paises)\n",
    "s = pd.Series(data=np.ones(158))\n",
    "np.fill_diagonal(df_nodes_features.values, s)\n",
    "\n",
    "#Se podría probar a que fuese 158x1, con un 0 o 1 si tiene conexión o no. \n",
    "#El pais se fija en el index y al propia red debería abstarer esa información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de características de los nodos 158x1.\n",
    "\n",
    "#Lista con todos los Node Features.\n",
    "lista_df_node_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    arrayBase = np.zeros((158,1))\n",
    "    df_nodes = pd.DataFrame(data = arrayBase, columns = ['Feature'])\n",
    "\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "\n",
    "    #Rellenamos con 1s donde exista el país\n",
    "    for i in range(LenCO):\n",
    "        if(LenCO != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(LenCC):\n",
    "        if(LenCC != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "    for i in range(LenCD):\n",
    "        if(LenCD != 0):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCD[i]] == True]\n",
    "            df_nodes.iloc[indexOrigen] = 1\n",
    "        \n",
    "    #Insertamos el df en la lista de df_node_features.\n",
    "    lista_df_node_features.append(df_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los Edge Features -> 1 por grafo. (Representa la info de cada conexión).\n",
    "#Columnas = Source, target y producto en one hot encoding. \n",
    "#Filas = nº de conexiones. \n",
    "\n",
    "#Lista con todos los Edge Features.\n",
    "lista_df_edge_features = []\n",
    "\n",
    "#Bucle que recorra todos los registros, y relacione los nodos de df_nodes entre sí, marcando la relación entre sus índices.\n",
    "#Además añadimos la x generada anteriormente.\n",
    "for index, row in df.iterrows():\n",
    "    #Creamos el df vacío, al que le añadimos las columnas source y target.\n",
    "    df_edges = pd.DataFrame(columns = x.columns)\n",
    "    df_edges.insert (0, \"target\", np.nan)\n",
    "    df_edges.insert (0, \"source\", np.nan)\n",
    "    #Guardamos las longitudes de cada registro (nº de países). Si es vacío, establecemos un 0. \n",
    "    #LenCO = Country origen. LenCC = Country Concern y LenCD = Contry Destin.\n",
    "    if(not row['COUNT_ORIGEN'] == \"\" and not row['COUNT_ORIGEN'] == \" \"):\n",
    "        LenCO = len(row['COUNT_ORIGEN'].split(\",\"))\n",
    "        paisesCO = row['COUNT_ORIGEN'].split(\",\")\n",
    "    else:\n",
    "        LenCO = 0\n",
    "    if(not row['COUNT_CONCERN'] == \"\" and not row['COUNT_CONCERN'] == \" \"):\n",
    "        LenCC = len(row['COUNT_CONCERN'].split(\",\"))\n",
    "        paisesCC = row['COUNT_CONCERN'].split(\",\")\n",
    "    else:\n",
    "        LenCC = 0\n",
    "    if(not row['COUNT_DESTIN'] == \"\" and not row['COUNT_DESTIN'] == \" \"):\n",
    "        LenCD = len(row['COUNT_DESTIN'].split(\",\"))\n",
    "        paisesCD = row['COUNT_DESTIN'].split(\",\")\n",
    "    else:\n",
    "        LenCD = 0 \n",
    "    #Creamos dos bucles anidados, para recorrer todos los source y linkarlos con todos los destinos. \n",
    "    #¡Esto funciona porque los índices de df_nodes siguen este mismo orden! \n",
    "    #Pero OJO! -> Existen 3 posibles casos: 1) LenCO = 0, 2) LenCC = 0 y 3) LenCD = 0. Teóricamente, habría que hacer 1->2->3.\n",
    "    #Si los 3 existen y son mayores que 0 se ejecutan los dos bucles.\n",
    "    \n",
    "    #Empezamos a rellenar el df, si existe LenCO y LenCC, lo rellenamos. \n",
    "    if(LenCO != 0 and LenCC != 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCC):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCC[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Si LenCD es 0, no pasa nada, el grafo ya está creado. Si sí que existe, seguimos rellenando el grafo.\n",
    "    if(LenCC != 0 and LenCD != 0):\n",
    "        for i in range(LenCC):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCC[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "                \n",
    "    #Por aquí solo va a entrar si no ha entrado en los otros dos.\n",
    "    if(LenCC == 0):\n",
    "        for i in range(LenCO):\n",
    "            indexOrigen = df_nodes_features.index[df_nodes_features[paisesCO[i]] == True]\n",
    "            for j in range(LenCD):\n",
    "                indexDestino = df_nodes_features.index[df_nodes_features[paisesCD[j]] == True]\n",
    "                serie = pd.Series([indexOrigen[0]] + [indexDestino[0]] + [np.float32(x.iloc[index][0])]) #.append(x.iloc[index])\n",
    "                df_edges.loc[df_edges.shape[0]] = serie.values\n",
    "    \n",
    "    #Insertamos el df en la lista de df_edge_features.\n",
    "    lista_df_edge_features.append(df_edges)\n",
    "\n",
    "#Convertimos la columna de weights a numérico, para que no de errores al crear los grafos de Stellargraph.\n",
    "for i in range(len(lista_df_edge_features)):\n",
    "    lista_df_edge_features[i].weight = pd.to_numeric(lista_df_edge_features[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROD_CAT         food contact materials\n",
       "COUNT_ORIGEN                  Hong Kong\n",
       "COUNT_CONCERN               China,Italy\n",
       "COUNT_DESTIN                           \n",
       "Name: 50, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "..       ...\n",
       "153      0.0\n",
       "154      0.0\n",
       "155      0.0\n",
       "156      0.0\n",
       "157      0.0\n",
       "\n",
       "[158 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  target  weight\n",
       "0    63.0    27.0    13.0\n",
       "1    63.0    74.0    13.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplos para enseñar\n",
    "miniIndex = 50\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(df.iloc[miniIndex])\n",
    "display(lista_df_node_features[miniIndex])\n",
    "display(lista_df_edge_features[miniIndex])\n",
    "graph_labels[miniIndex]\n",
    "#y_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista con todos los StellarGraphs, pero los agrupamos de 10 en 10, hasta que se acabe una clase \n",
    "\n",
    "graph_labels_reducida = []  #Donde vamso a guardar los labels de los grafos resultantes.\n",
    "lista_grafos = []           #Donde vamos a guardar los grafos resultantes.\n",
    "posiciones = []             #Guardamos las posiciones de los ultimos 10 grafos con la msima amenaza\n",
    "\n",
    "#Cuantos grafos queremos agrupar, si tenemos menos de n registros de esa amenaza dará error.\n",
    "numGraphsCombined = 10\n",
    "\n",
    "#No es el más óptimo, pero funciona.\n",
    "for i in range(len(graph_labels.unique())):\n",
    "    cont = 0\n",
    "    df_nodes_prov = pd.DataFrame(data = np.zeros(158), columns = [\"Feature\"])\n",
    "    df_edge_features_prov = pd.DataFrame(columns = [\"source\", \"target\", \"weight\"])\n",
    "    for j in range(len(lista_df_edge_features)):\n",
    "        if(graph_labels[j] == i):\n",
    "            cont = cont + 1\n",
    "            df_nodes_prov = df_nodes_prov.combine(lista_df_node_features[j],np.maximum)\n",
    "            df_edge_features_prov = df_edge_features_prov.append(lista_df_edge_features[j],ignore_index=True)\n",
    "            posiciones.append(j)\n",
    "            if(cont%numGraphsCombined == 0):            \n",
    "                #Insertamos el grafo de \"numGraphsCombined\" (combinado)\n",
    "                lista_grafos.append(sg.StellarDiGraph(nodes = df_nodes_prov, edges = df_edge_features_prov, node_type_default=\"Paises\", edge_type_default=\"Conexiones\"))\n",
    "                graph_labels_reducida.append(i)\n",
    "                #Reseteamos los df auxiliares\n",
    "                df_nodes_prov = pd.DataFrame(data = np.zeros(158), columns = [\"Feature\"])\n",
    "                df_edge_features_prov = pd.DataFrame(columns = [\"source\", \"target\", \"weight\"])\n",
    "    if(cont%10!=0):\n",
    "        lista_grafos.append(sg.StellarDiGraph(nodes = df_nodes_prov, edges = df_edge_features_prov, node_type_default=\"Paises\", edge_type_default=\"Conexiones\"))\n",
    "        graph_labels_reducida.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle de lista_grafos y graph_labels_reducida\n",
    "\n",
    "ObjetoProvisional = list(zip(lista_grafos, graph_labels_reducida))\n",
    "\n",
    "random.shuffle(ObjetoProvisional)\n",
    "\n",
    "lista_grafos, graph_labels_reducida = zip(*ObjetoProvisional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(lista_grafos))\n",
    "display(len(graph_labels_reducida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarDiGraph: Directed multigraph\n",
      " Nodes: 158, Edges: 21\n",
      "\n",
      " Node types:\n",
      "  Paises: [158]\n",
      "    Features: float32 vector, length 1\n",
      "    Edge types: Paises-Conexiones->Paises\n",
      "\n",
      " Edge types:\n",
      "    Paises-Conexiones->Paises: [21]\n",
      "        Weights: range=[7, 23], mean=16.7143, std=5.34923\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "#Visualizamos info de un grafo\n",
    "print(lista_grafos[8].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>2223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>158.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>158.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>158.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>158.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nodes   edges\n",
       "count  2223.0  2223.0\n",
       "mean    158.0    19.0\n",
       "std       0.0     9.2\n",
       "min     158.0     9.0\n",
       "25%     158.0    13.0\n",
       "50%     158.0    16.0\n",
       "75%     158.0    22.0\n",
       "max     158.0   125.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Información acerca de los grafos obtenidos.\n",
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges()) for g in lista_grafos],\n",
    "    columns=[\"nodes\", \"edges\"],\n",
    ")\n",
    "summary.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lista_grafos' (tuple)\n",
      "Stored 'graph_labels_reducida' (tuple)\n",
      "Stored 'y_guide' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store lista_grafos\n",
    "%store graph_labels_reducida\n",
    "%store y_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
